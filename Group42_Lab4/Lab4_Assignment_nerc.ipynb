{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfcHuFqq1lxA",
    "outputId": "0d48fc6b-765f-4373-f740-f7907231ddbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3HCRXnw21Y2",
    "outputId": "42dc220a-7394-4bbf-b77a-7e112693880b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  CONLL2003.zip\n",
      "   creating: CONLL2003/\n",
      "  inflating: __MACOSX/._CONLL2003    \n",
      "  inflating: CONLL2003/train.txt     \n",
      "  inflating: __MACOSX/CONLL2003/._train.txt  \n",
      "  inflating: CONLL2003/README.MD     \n",
      "  inflating: __MACOSX/CONLL2003/._README.MD  \n",
      "  inflating: CONLL2003/valid.txt     \n",
      "  inflating: __MACOSX/CONLL2003/._valid.txt  \n",
      "  inflating: CONLL2003/test.txt      \n",
      "  inflating: __MACOSX/CONLL2003/._test.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip CONLL2003.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XmYVsrG1kJ0"
   },
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88_oQ1ow1kJ4"
   },
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlYotLim1kJ6"
   },
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UQbS1n_1kJ7"
   },
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0Vorhmj1kJ9",
    "outputId": "80e4bf4d-6533-4cbe-b0de-d9ed590b7708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': 'rejects', 'pos': 'VBZ'}\n",
      "203621\n",
      "203621\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/content/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "       # add features\n",
    "        'words': token, \n",
    "        'pos': pos\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)\n",
    "    \n",
    "print(training_features[1])\n",
    "print(len(training_features))\n",
    "print(len(training_gold_labels))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eNfUuPGK1kKB"
   },
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/content/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "        # add features\n",
    "        'words': token, \n",
    "        'pos': pos\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeOakTBV1kKB"
   },
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test? 203621 in train, 46435 in test.\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur? See Counter output below\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, the data is not balanced as we can observe that most are labeled as label 'O' and the other entity tokens are between 1155 for 'I-MISC' Miscellaneous entities and 7140 for 'B-LOC' Location entities at the beginning of a chunk of data.\n",
    "For the test set, the data is not balanced as we can observe that most are labeled as label 'O', the rest of the entity tokens are between 216 for 'I-MISC' Miscellaneous entities and 1668 for 'B-LOC' Location entities. \n",
    "\n",
    "However, ratio-wise, each entity is occurring in a similar percentage in both the training set and the test set which contributes to some extent of balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621\n",
      "203621\n",
      "46435\n",
      "46435\n"
     ]
    }
   ],
   "source": [
    "print(len(training_features))\n",
    "print(len(training_gold_labels))  \n",
    "print(len(test_features))\n",
    "print(len(test_gold_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Pvg8egV1kKD",
    "outputId": "d8d81207-1456-42c4-e2b0-27b1cd9810ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 169578, 'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155})\n",
      "Counter({'O': 38323, 'B-LOC': 1668, 'B-ORG': 1661, 'B-PER': 1617, 'I-PER': 1156, 'I-ORG': 835, 'B-MISC': 702, 'I-LOC': 257, 'I-MISC': 216})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "my_list=[1,2,1,3,2,5]\n",
    "Counter(my_list)\n",
    "print(Counter(training_gold_labels))\n",
    "print(Counter(test_gold_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpBu_xPP1kKD"
   },
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r13bDhmN1kKE"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5v6iGFPr1kKF",
    "outputId": "d2852d47-9321-4f7b-e289-3af3c59f5b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training words = (203621, 27361)\n",
      "Number of test words = (46435, 27361)\n"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "the_array = training_features + test_features # your code here\n",
    "the_array = vec.fit_transform(the_array).toarray()\n",
    "the_array.shape\n",
    "\n",
    "train_feats = the_array[:len(training_features)]\n",
    "test_feats = the_array[len(training_features):]\n",
    "print('Number of training words =', train_feats.shape)\n",
    "print('Number of test words =', test_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30TPDMXy1kKF"
   },
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "\n",
    "The classifier performs especially well on the NERC label 'O' with a recall, precision, and f-1 score of 0.98. The classifier performs quite well on the NERC label 'B-LOC' with a recall of 0.81, precision of 0.78, and f-1 score of 0.79. This is the case due to the fact that these labels had the most data for the training set.\n",
    "\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?\n",
    "\n",
    "\n",
    "The classifier performs poorly on the NERC label 'I-MISC' with a recall of 0.59, precision of 0.57, and f-1 score of 0.58. Additionally, the classifier performs poorly on the NERC label 'I-LOC' with a recall of 0.62, precision of 0.53, and f-1 score of 0.57. This is the case because these labels have the lowest amount of training data for. This lack of training data is effecting negatively the performance of the classifier. Additionally, it is noticeable that the beginning of a chuck (B-...) have higher scores that the following parts of an entity (I-...). More training data for I- labels would probably have a positive effect on the results of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AZQZKX4m1kKG"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "q78PoW_e1kKG"
   },
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "qwzeipG51kKG",
    "outputId": "9d11611a-8538-4948-cf4d-71f1c9105f07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "lin_clf.fit(train_feats, training_gold_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXK8Loq1VEQE",
    "outputId": "9cf66201-4679-4a84-9c08-7c52b3eac288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.78      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.72       702\n",
      "       B-ORG       0.79      0.52      0.63      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.57      0.59      0.58       216\n",
      "       I-ORG       0.70      0.47      0.56       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.98      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.72      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_gold_labels, lin_clf.predict(test_feats)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASiPXJ-41kKH"
   },
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ocIErS-n7gkk"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import spacy\n",
    "##### Adapt the path to point to your local copy of the Google embeddings model\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "op7n18uVszQs"
   },
   "outputs": [],
   "source": [
    "train = ConllCorpusReader('/content/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "H65KzBS47gtJ"
   },
   "outputs": [],
   "source": [
    "input_vectors=[]\n",
    "labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "9RZOH188uBAC",
    "outputId": "e4f53c0b-ab4a-4aa3-8f17-5f281c70685f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf2 = svm.LinearSVC()\n",
    "lin_clf2.fit(input_vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kmT6H-_vs0yE"
   },
   "outputs": [],
   "source": [
    "test = ConllCorpusReader('/content/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UKGDuUSN1kKH"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "input_vectors=[]\n",
    "labels=[]\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC0lDXqWu_l8",
    "outputId": "c07edd08-3cd1-4ac7-cd78-38e2705406f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, lin_clf2.predict(input_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the features with the highest performance are O and B-LOC. However, the classifier that uses the embedded model performs worse on organizations (especially the I labels) now and again the I labels for the locations. Moreover, looking at the overall precision, recall and f1 scores, the classifier using the embeddings model tends to have different scores that vary from entity to entity where they are either a bit higher or a bit lower than the model in 1d. The accuracy of this model heavily depends on the words that are available within the genism word embedding model, as those that are not included will be filled with zero vectors. The more this happens, the worse it will most likely perform for those word entity classes. Finally, looking at the total macro and weighted averages and the total accuracy scores, we see that in general the performance goes down for most scores but not in any significant way, and for accuracy it actually increased by 0.01 (0.92->0.93). For these reasons, we cannot say that is a considerable difference in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqIcGGQr1kKH"
   },
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "URLUy6EM1kKI"
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gM8xr6VfzBRx",
    "outputId": "e9586623-bee1-4378-a8f4-c90770898350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='/content/ner.csv' mode='r' encoding='utf-8'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/ner.csv\", encoding=\"utf-8\") as file:\n",
    "  print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jH9kz8UK1kKI",
    "outputId": "ae3f521b-8bb4-4056-9446-2177915f067d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = \"/content/ner.csv\"\n",
    "kaggle_dataset = pandas.read_csv(path, encoding='cp1252', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rEWQg2c1kKI",
    "outputId": "00d6c9a6-8b7f-4cfc-d33c-f04e06c6841d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsC4GFO-1kKJ",
    "outputId": "f045d5e0-59ba-4a2d-a0f7-e5b3ac0996b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxnjValuDMmJ",
    "outputId": "8fc52197-4bf6-4bb4-dbce-e0375a2032ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n",
       "       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n",
       "       'next-word', 'pos', 'prev-iob', 'prev-lemma', 'prev-pos',\n",
       "       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n",
       "       'prev-prev-word', 'prev-shape', 'prev-word', 'sentence_idx', 'shape',\n",
       "       'word', 'tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDxdU8vYD5qZ",
    "outputId": "f862d95f-1697-4f67-c23a-0b8f8a0b4094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Unnamed: 0                     0\n",
      "lemma                   thousand\n",
      "next-lemma                    of\n",
      "next-next-lemma         demonstr\n",
      "next-next-pos                NNS\n",
      "next-next-shape        lowercase\n",
      "next-next-word     demonstrators\n",
      "next-pos                      IN\n",
      "next-shape             lowercase\n",
      "next-word                     of\n",
      "pos                          NNS\n",
      "prev-iob              __START1__\n",
      "prev-lemma            __start1__\n",
      "prev-pos              __START1__\n",
      "prev-prev-iob         __START2__\n",
      "prev-prev-lemma       __start2__\n",
      "prev-prev-pos         __START2__\n",
      "prev-prev-shape         wildcard\n",
      "prev-prev-word        __START2__\n",
      "prev-shape              wildcard\n",
      "prev-word             __START1__\n",
      "sentence_idx                 1.0\n",
      "shape                capitalized\n",
      "word                   Thousands\n",
      "tag                            O\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for index, instance in kaggle_dataset.iterrows():\n",
    "    print(index)\n",
    "    print(instance) # you can access information by using instance['A COLUMN NAME'] which you can use to convert to a dictionary needed for the feature representation.\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXc16zgNDe4G",
    "outputId": "590439d0-fb86-43fa-867c-a0a8b81a32b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemma': 'of', 'next-lemma': 'demonstr', 'next-next-lemma': 'have', 'next-next-pos': 'VBP', 'next-next-shape': 'lowercase', 'next-next-word': 'have', 'next-pos': 'NNS', 'next-shape': 'lowercase', 'next-word': 'demonstrators', 'pos': 'IN', 'prev-iob': 'O', 'prev-lemma': 'thousand', 'prev-pos': 'NNS', 'prev-prev-iob': '__START1__', 'prev-prev-lemma': '__start1__', 'prev-prev-pos': '__START1__', 'prev-prev-shape': 'wildcard', 'prev-prev-word': '__START1__', 'prev-shape': 'capitalized', 'prev-word': 'Thousands', 'sentence_idx': 1.0, 'shape': 'lowercase', 'word': 'of'}\n",
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for index, instance in df_train.iterrows():\n",
    "    a_dict = {\n",
    "       # add features\n",
    "        'lemma': instance['lemma'],\n",
    "        'next-lemma': instance['next-lemma'],                   \n",
    "        'next-next-lemma': instance['next-next-lemma'],\n",
    "        'next-next-pos': instance['next-next-pos'], \n",
    "        'next-next-shape': instance['next-next-shape'],\n",
    "        'next-next-word': instance['next-next-word'],\n",
    "        'next-pos': instance['next-pos'],                     \n",
    "        'next-shape': instance['next-shape'],\n",
    "        'next-word': instance['next-word'],\n",
    "        'pos': instance['pos'],\n",
    "        'prev-iob': instance['prev-iob'],\n",
    "        'prev-lemma': instance['prev-lemma'],\n",
    "        'prev-pos': instance['prev-pos'],\n",
    "        'prev-prev-iob': instance['prev-prev-iob'],\n",
    "        'prev-prev-lemma': instance['prev-prev-lemma'],\n",
    "        'prev-prev-pos': instance['prev-prev-pos'],\n",
    "        'prev-prev-shape': instance['prev-prev-shape'],\n",
    "        'prev-prev-word': instance['prev-prev-word'],\n",
    "        'prev-shape': instance['prev-shape'],\n",
    "        'prev-word': instance['prev-word'],\n",
    "        'sentence_idx': instance['sentence_idx'],\n",
    "        'shape': instance['shape'],\n",
    "        'word': instance['word']\n",
    "        }\n",
    "    train_features.append(a_dict)\n",
    "    train_labels.append(instance['tag'])\n",
    "    \n",
    "print(train_features[1])\n",
    "print(len(train_features))\n",
    "print(len(train_labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IR7KHoDvME7z",
    "outputId": "3eb43316-52a8-4c86-b8f5-bd5ef2345568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemma': 'death', 'next-lemma': 'to', 'next-next-lemma': 'america', 'next-next-pos': 'NNP', 'next-next-shape': 'capitalized', 'next-next-word': 'America', 'next-pos': 'TO', 'next-shape': 'lowercase', 'next-word': 'to', 'pos': 'NN', 'prev-iob': 'O', 'prev-lemma': '\"', 'prev-pos': '``', 'prev-prev-iob': 'O', 'prev-prev-lemma': 'chant', 'prev-prev-pos': 'VBG', 'prev-prev-shape': 'lowercase', 'prev-prev-word': 'chanting', 'prev-shape': 'punct', 'prev-word': '\"', 'sentence_idx': 4544.0, 'shape': 'capitalized', 'word': 'Death'}\n",
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "for index, instance in df_test.iterrows():\n",
    "    a_dict = {\n",
    "       # add features\n",
    "        'lemma': instance['lemma'],\n",
    "        'next-lemma': instance['next-lemma'],                   \n",
    "        'next-next-lemma': instance['next-next-lemma'],\n",
    "        'next-next-pos': instance['next-next-pos'], \n",
    "        'next-next-shape': instance['next-next-shape'],\n",
    "        'next-next-word': instance['next-next-word'],\n",
    "        'next-pos': instance['next-pos'],                     \n",
    "        'next-shape': instance['next-shape'],\n",
    "        'next-word': instance['next-word'],\n",
    "        'pos': instance['pos'],\n",
    "        'prev-iob': instance['prev-iob'],\n",
    "        'prev-lemma': instance['prev-lemma'],\n",
    "        'prev-pos': instance['prev-pos'],\n",
    "        'prev-prev-iob': instance['prev-prev-iob'],\n",
    "        'prev-prev-lemma': instance['prev-prev-lemma'],\n",
    "        'prev-prev-pos': instance['prev-prev-pos'],\n",
    "        'prev-prev-shape': instance['prev-prev-shape'],\n",
    "        'prev-prev-word': instance['prev-prev-word'],\n",
    "        'prev-shape': instance['prev-shape'],\n",
    "        'prev-word': instance['prev-word'],\n",
    "        'sentence_idx': instance['sentence_idx'],\n",
    "        'shape': instance['shape'],\n",
    "        'word': instance['word'] \n",
    "        }\n",
    "    test_features.append(a_dict)\n",
    "    test_labels.append(instance['tag'])\n",
    "    \n",
    "print(test_features[1])\n",
    "print(len(test_features))\n",
    "print(len(test_labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLaLtkKbCwIr",
    "outputId": "0b20c5b8-36a1-427d-aac5-730b10649feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training words = (100000, 98137)\n",
      "Number of test words = (20000, 98137)\n"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "the_array = train_features + test_features # your code here\n",
    "the_array = vec.fit_transform(the_array).toarray()\n",
    "the_array.shape\n",
    "\n",
    "train_feats = the_array[:len(train_features)]\n",
    "test_feats = the_array[len(train_features):]\n",
    "print('Number of training words =', train_feats.shape)\n",
    "print('Number of test words =', test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "DW4M9l06SXB2",
    "outputId": "5c769e0c-95d9-41fd-e4e2-4d6f9af04cd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf3 = svm.LinearSVC()\n",
    "lin_clf3.fit(train_feats, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUAYHVoU1kKJ"
   },
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CA_XGf2sTHYZ",
    "outputId": "4e9f3d2c-1c11-4df0-90b1-0d12da5f2449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         4\n",
      "       B-geo       0.88      0.25      0.39       741\n",
      "       B-gpe       0.26      0.97      0.42       296\n",
      "       B-nat       0.00      0.00      0.00         8\n",
      "       B-org       0.37      0.68      0.48       397\n",
      "       B-per       0.62      0.67      0.64       333\n",
      "       B-tim       1.00      0.06      0.12       393\n",
      "       I-geo       0.65      0.90      0.75       156\n",
      "       I-gpe       0.00      0.00      0.00         2\n",
      "       I-nat       0.00      0.00      0.00         4\n",
      "       I-org       0.00      0.00      0.00       321\n",
      "       I-per       0.70      0.99      0.82       319\n",
      "       I-tim       0.49      0.44      0.47       108\n",
      "           O       0.99      0.98      0.98     16918\n",
      "\n",
      "    accuracy                           0.90     20000\n",
      "   macro avg       0.43      0.43      0.36     20000\n",
      "weighted avg       0.93      0.90      0.90     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Code below gives warning as output because the classifier did not predict some of the true labels based on the test features \n",
    "print(classification_report(test_labels, lin_clf3.predict(test_feats)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision values range from 0 to 1, with a precision of 1 indicating that there are no false positives. On the other hand, a precision of 0 indicates that the model was incapable of making correct positive predictions.\n",
    "When looking at the precision values of these models, the majority of the classes had low values, such as B-art, B-gpe, B-nat, B-org, I-gpe, I-nat, I-org, I-tim. The classes mentioned had precision values that were below 0.5, thus illustrating that they were incapable of correctly predicting positive instances. However, B-geo, B-per, I-per, I-geo and O had high precision values. Thus, we can assume that this means that the models that had a precision value above 0.5 were capable of predicting positive instances to a certain extent.\n",
    "\n",
    "Recall values range from 0 to 1, with a recall of 1 indicating that the predictions are true positive. A recall rate of 0 indicates that the predictions were unable to identify any true positives.\n",
    "The recall values are fairly spread, in comparison to the precision rates. B-gpe, I-per, I-geo and O had extremely high recall values, thus the model was able to identify the majority of the positive instances. B-art, B-geo, B-nat, B-tim, I-gpe, I-nat, I-org were unable to identify the positive instances.\n",
    "\n",
    "F1-scores range from 0 to 1, its a combination of the precision and recall value.\n",
    "Classes I-per, O, I-geo, B-geo, B-per and B-org have fairly high F1-scores meaning that these class models were able to indicate true positive instances. Moreover, it means that these class models had balanced out precision and recall values. For those that had a low F1-score means that the class model performed poorly in indicating the true positive instances. This could be due to having a heavier precision or recall value, causing an imbalance and the need to tune and optimize.\n",
    "\n",
    "The accuracy displays the accuracy of the model, which was 0.9 out of 1.0. This is a good accuracy rate, however the models performance is not in balance with the classes. The precision, recall and F1-scores have a large range and vary in their values, resulting in different performance levels for each class.\n",
    "\n",
    "The macro avg is the average for all classes for each column; precision, recall and F1-score. The values were 0.43, 0.43 and 0.36, thus the performance of the model in general for all classes were low. The weighted avg is the weighted average for all classes for each column ( precision, recall and F1-score ), which is then weighted by the number of instances in each class. The values were 0.93, 0.90 and 0.90, meaning that the performance of the model was in favor for classes that had many samples in their class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOF15KrU1kKJ"
   },
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
